# -*- coding: utf-8 -*-
"""MiniGpt_from_scratch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EjnxMPpW5s_OimqwmxP_1C8uON83Aq5p
"""

import torch
import matplotlib.pyplot as plt
import torch.nn as nn
import math
import torch.nn.functional as F

class GPTConfig:
  attn_dropout = 0.1
  embed_dropout = 0.1
  ff_dropout = 0.1

  def __init__(self , vocab_size , max_len, **kwargs):

    self.vocab_size = vocab_size
    self.max_len = max_len

    for key , value in kwargs.items():
      setattr(self , key , value)

class MinGptConfig(GPTConfig):
  num_heads = 12
  num_blocks = 12
  embed_dim  = 768

class MiniGpt(nn.Module):

  def __init__(self , config):
    super().__init__()
    embed_dim = config.embed_dim
    self.max_len =  config.max_len

    self.pos_embed = nn.Parameter(
        torch.zeros(1 , config.max_len , embed_dim)
    )
    self.token_embed = nn.Embedding(
        config.vocab_size , embed_dim
    )
    self.dropout = nn.Dropout(config.embed_dropout)
    self.blocks = nn.Sequential(
        *[Block(config) for _ in range(config.num_blocks)]
    )
    self.ln = nn.LayerNorm(embed_dim)
    self.fc = nn.Linear(embed_dim , config.vocab_size)

  def forward(self , x , target = None):
    seq_len = x.size(1)
    assert seq_len <= self.max_len , "Sequnce longer than model capacity"

    tok_embedding = self.tok_embed(x)
    pos_embedding = self.pos_embed[: , :seq_len , :]
    x = self.dropout(tok_embedding+pos_embedding)
    x = self.blocks(x)
    x = self.ln(x)
    x = self.fc(x)

    return x

# from torch.nn.modules.activation import MultiheadAttention

class Block(nn.Module):

  def __init__(self , config):
    super().__init__()

    embed_dim = config.embed_dim
    self.ln1 = nn.LayerNorm(embed_dim)
    self.ln2 = nn.LayerNorm(embed_dim)
    self.attn = MultiheadAttention(config)
    self.ff = nn.Sequential(
        nn.Linear(embed_dim , embed_dim * 4),
        nn.GELU(),
        nn.Linear(embed_dim*4 , embed_dim),
        nn.Dropout(config.ff_dropout),
    )

    def forward(self , x):
      x = x + self.attn(self.ln1(x))
      x = x + self.ff(self.ln2(x))

      return x

class MultiheadAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        embed_dim = config.embed_dim
        self.num_heads = config.num_heads
        assert embed_dim % self.num_heads == 0, "invalid heads and embedding dimension configuration"
        self.key = nn.Linear(embed_dim, embed_dim)
        self.value = nn.Linear(embed_dim, embed_dim)
        self.query = nn.Linear(embed_dim, embed_dim)
        self.proj = nn.Linear(embed_dim, embed_dim)
        self.attn_dropout = nn.Dropout(config.attn_dropout)
        self.proj_dropout = nn.Dropout(config.ff_dropout)
        self.register_buffer(
            "mask",
            torch.tril(torch.ones(config.max_len, config.max_len))
            .unsqueeze(0).unsqueeze(0)
        )

    def forward(self , x):

      batch_size = x.shape(0)
      seq_len = x.shape(1)

      k_t = self.key(x).reshape(batch_size , seq_len , self.num_heads , -1).permute(0 , 2 , 3 , 1 )
      v = self.value(x).resahpe(batch_size , seq_len , self.num_heads , -1).transpose(1, 2)
      q = self.query(x).resahpe(batch_size , seq_len , self.num_heads , -1).transpose(1, 2)

      attn = torch.matmul(q , k_t)/math.sqrt(q.size(-1))
      mask = self.mask[: , : , :seq_len , :seq_len]
      attn = self.masked_fill(mask==0 , float("-inf"))
      attn = self.attn_dropout(attn)
      attn = F.softmax(attn , dim = -1)
      y = torch.matmul(attn , v)
      y = y.transpose(1, 2)
      y = y.reshape(batch_size , seq_len , -1)
      y = self.proj_dropout(self.proj(y))

      return y

vocab_size = 100
max_len = 20
num_heads = 2
config = MinGptConfig(vocab_size , max_len)
model = MiniGpt(config)

model

